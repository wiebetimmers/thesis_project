{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline_RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, T, dataset = None):\n",
    "        super(Baseline_RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_hidden_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.recurrent_layer = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(input_size + hidden_size, output_size)\n",
    "        \n",
    "        self.f_0 = nn.Tanh()\n",
    "        self.f_t = nn.ReLU(inplace=False)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.T = T\n",
    "        \n",
    "        # Initialize the weights\n",
    "        self.input_hidden_layer.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.recurrent_layer.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.output_layer.weight.data.uniform_(-0.01, 0.01)\n",
    "        \n",
    "        self.dataset = dataset\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        # Initialize hidden input & hidden layer\n",
    "        initial_hidden = torch.zeros(input.size()[0], self.hidden_size)\n",
    "        hidden = self.f_0(self.input_hidden_layer(initial_hidden))\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            if self.dataset == 'Digits':  # So we use the Digits dataset\n",
    "                combined = torch.cat((input, hidden), 1)  \n",
    "                hidden = self.f_t(self.recurrent_layer(combined))\n",
    "                \n",
    "            elif self.dataset == 'MNIST': # So we use the MNIST dataset\n",
    "                \n",
    "                # Squeeze the MNIST Tensor in 2 dimensions \n",
    "                three_d_tensor = input.squeeze(1)\n",
    "                two_d_tensor = three_d_tensor.contiguous().view(three_d_tensor.size()[0], -1)  # 28 * 28 pixels = 784\n",
    "                \n",
    "                combined = torch.cat((two_d_tensor, hidden), 1)\n",
    "                hidden = self.f_t(self.recurrent_layer(combined))\n",
    "        \n",
    "        \n",
    "        output = self.output_layer(combined)\n",
    "        y = self.softmax(output)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reservoir_RNN(nn.Module):\n",
    "    def __init__(self, input_size, reservoir_size, output_size, T, dataset = None):\n",
    "        super(Reservoir_RNN, self).__init__()\n",
    "        \n",
    "        # Activation functions\n",
    "        self.f_0 = nn.Tanh()\n",
    "        self.f_t = nn.ReLU(inplace=False)\n",
    "        self.f_y = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        # Amount of timesteps / recurrent layers\n",
    "        self.T = T\n",
    "        \n",
    "        # Initialize the weights & layers\n",
    "        self.initWeights(input_size, reservoir_size, output_size)\n",
    "        self.initLayers(input_size, reservoir_size, output_size)\n",
    "        \n",
    "        # Either digits or mnist\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        used_input = input\n",
    "        \n",
    "        # Squeeze the used input in 2 dims if we use the MNIST dataset.\n",
    "        if self.dataset == 'MNIST':\n",
    "            three_d_tensor = used_input.squeeze(1)\n",
    "            used_input = three_d_tensor.contiguous().view(three_d_tensor.size()[0], -1)  # 28 * 28 pixels = 784\n",
    "        \n",
    "        # Calculate c_0\n",
    "        c = self.f_0(self.layer1(used_input))\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            # c_t =  f_t (W_r * c_t-1 + U * x_t)\n",
    "            c = self.f_t(self.layer2(c) + self.layer3(used_input))\n",
    "        \n",
    "        # Calculate y = f_y ( W_out * c_t)\n",
    "        y = self.f_y(self.layer4(c))\n",
    "    \n",
    "        return y\n",
    "    \n",
    "    def initWeights(self, input_size, reservoir_size, output_size):\n",
    "        \n",
    "        # Sample the initial weights from a uniform distribution - initialize the same as in the baseline model.\n",
    "        self.W_in = nn.Parameter(data = torch.zeros(reservoir_size, input_size, requires_grad=False))\n",
    "        self.W_in.data.uniform_(-0.01, 0.01)\n",
    "        \n",
    "        self.W_r = nn.Parameter(data = torch.zeros(reservoir_size, reservoir_size), requires_grad=False)\n",
    "        self.W_r.data.uniform_(-0.01, 0.01)\n",
    "        \n",
    "        self.W_out = nn.Parameter(data = torch.zeros(output_size, reservoir_size), requires_grad=True)\n",
    "        self.W_out.data.uniform_(-0.01, 0.01)\n",
    "        \n",
    "        self.U = nn.Parameter(data = torch.zeros(reservoir_size, input_size), requires_grad=False)\n",
    "        self.U.data.uniform_(-0.01, 0.01)\n",
    "        return\n",
    "    \n",
    "    def initLayers(self, input_size, reservoir_size, output_size):\n",
    "        # Input layer\n",
    "        self.layer1 = torch.nn.Linear(input_size, reservoir_size, bias=True)\n",
    "        self.layer1.weight = self.W_in\n",
    "        self.layer1.weight.requires_grad = False\n",
    "        self.layer1.bias.requires_grad = False\n",
    "        \n",
    "        # Recurrent layer\n",
    "        self.layer2 = torch.nn.Linear(reservoir_size, reservoir_size, bias=True)\n",
    "        self.layer2.weight = self.W_r\n",
    "        self.layer2.bias.requires_grad = False\n",
    "        self.layer3 = torch.nn.Linear(input_size, reservoir_size, bias=True)\n",
    "        self.layer3.weight = self.U\n",
    "        self.layer3.bias.requires_grad = False\n",
    "        \n",
    "        # Output layer\n",
    "        self.layer4 = torch.nn.Linear(reservoir_size, output_size, bias=True)\n",
    "        self.layer4.weight = self.W_out\n",
    "        self.layer4.bias.requires_grad = True\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
